{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sample_size, pathX, pathY):\n",
    "    f = gzip.open(pathX, 'r')\n",
    "\n",
    "    # read off unimportant bytes describing file protocol\n",
    "    image_size = 28 * 28\n",
    "    protocol_length = 16\n",
    "    f.read(protocol_length)\n",
    "\n",
    "    X = f.read(image_size * sample_size)\n",
    "    X = np.frombuffer(X, dtype=np.uint8).astype(np.float32)\n",
    "    X = X.reshape(sample_size, image_size)\n",
    "\n",
    "    f = gzip.open(pathY, 'r')\n",
    "\n",
    "    protocol_length = 8\n",
    "    f.read(protocol_length)\n",
    "    Y_temp = f.read(sample_size)\n",
    "    Y_temp = np.frombuffer(Y_temp, dtype=np.uint8)\n",
    "\n",
    "    Y = np.zeros([sample_size, 10], dtype='f')\n",
    "    for sample in range(sample_size):\n",
    "        Y[sample][Y_temp[sample]] = 1.0\n",
    "\n",
    "    # return [np.array([data / 255.0 for data in X], dtype='f'), Y]\n",
    "    return [np.array([data / 127.5 for data in X], dtype='f') - 1.0, Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        for layer in range(1, len(layers)):\n",
    "            self.w.append((2.0 * (np.random.randint(1e9, size=[layers[layer], layers[layer - 1]]) / 1e9)) - 1.0)\n",
    "            self.b.append(((2.0 * (np.random.randint(1e9, size=[layers[layer]]) / 1e9)) - 1.0).reshape([layers[layer], 1]))\n",
    "\n",
    "    def sigmoid(self, data):\n",
    "        return np.where(data >= 0, 1 / (1 + np.exp(-data)), np.exp(data) / (1 + np.exp(data)))\n",
    "\n",
    "    def sigmoid_derivative(self, data):\n",
    "        temp = self.sigmoid(data)\n",
    "        return temp * (1 - temp)\n",
    "\n",
    "    def tanh(self, data):\n",
    "        return np.tanh(data)\n",
    "    \n",
    "    def tanh_derivative(self, data):\n",
    "        return 1 - np.tanh(data) ** 2\n",
    "\n",
    "    def backPropFast(self, train_X, train_Y, eta):\n",
    "        m = len(train_X[0])\n",
    "\n",
    "        gradients_w = [np.zeros([self.layers[layer], self.layers[layer - 1]], dtype='f') for layer in range(1, len(self.layers))]\n",
    "        gradients_b = [np.zeros([self.layers[layer]], dtype='f') for layer in range(1, len(self.layers))]\n",
    "        activations = [train_X]\n",
    "        zs = [train_X]\n",
    "\n",
    "        for layer in range(1, len(self.layers)):\n",
    "            zs.append(np.dot(self.w[layer - 1], activations[-1]) + self.b[layer - 1])\n",
    "\n",
    "            if layer == len(self.layers) - 1: activations.append(self.sigmoid(zs[-1]))\n",
    "            else: activations.append(self.tanh(zs[-1]))\n",
    "\n",
    "        # print(\"max:\", np.max((activations[-1] + 1.0) / 2.0))\n",
    "        # print(\"min:\", np.min((activations[-1] + 1.0) / 2.0))\n",
    "\n",
    "        delta_l = (activations[-1] - train_Y) * self.sigmoid_derivative(zs[-1])\n",
    "        gradients_b[-1] = delta_l.sum(axis=1)\n",
    "        gradients_w[-1] = np.dot(delta_l, np.transpose(activations[-2]))\n",
    "\n",
    "        for layer in range(len(self.layers) - 2, 0, -1):\n",
    "            delta_l = np.dot(np.transpose(self.w[layer]), delta_l) * self.tanh_derivative(zs[layer])\n",
    "            gradients_b[layer - 1] += delta_l.sum(axis=1)\n",
    "            gradients_w[layer - 1] += np.dot(delta_l, np.transpose(activations[layer - 1]))\n",
    "        \n",
    "        for layer in range(len(gradients_b)):\n",
    "            self.w[layer] -= eta * gradients_w[layer] / float(m)\n",
    "            self.b[layer] -= eta * gradients_b[layer].reshape([len(gradients_b[layer]), 1]) / float(m)\n",
    "\n",
    "    def train(self, train_X, train_Y, eta, batch_size):\n",
    "        m = len(train_X[0]) #60000\n",
    "        for batch in range(int(m / batch_size)):\n",
    "            X = train_X[ : , batch * batch_size : (batch + 1) * batch_size]\n",
    "            Y = train_Y[ : , batch * batch_size : (batch + 1) * batch_size]\n",
    "            self.backPropFast(X, Y, eta)\n",
    "\n",
    "    def getAccuracy(self, test_X, test_Y):\n",
    "        for layer in range(len(self.layers) - 1):\n",
    "            if layer == len(self.layers) - 2:\n",
    "                test_X = self.sigmoid(np.dot(self.w[layer], test_X) + self.b[layer].reshape([self.layers[layer + 1], 1]))\n",
    "            else: test_X = self.tanh(np.dot(self.w[layer], test_X) + self.b[layer].reshape([self.layers[layer + 1], 1]))\n",
    "        # print(test_X.shape)\n",
    "        # print(np.max(test_X, axis=0).shape)\n",
    "        # print(np.max(test_X, axis=0)[0])\n",
    "        # print(np.min(test_X, axis=0)[0])\n",
    "        # exit(0)\n",
    "        # test_X = (test_X + 1.0) / 2.0\n",
    "        predictions = np.argmax(test_X, 0)\n",
    "        Y = np.argmax(test_Y, 0)\n",
    "        print(\"Accuracy:\", str(np.sum(predictions == Y) * 100.0 / float(len(predictions))) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "train_X, train_Y = get_data(train_size, 'MNIST_Dataset/train-images-idx3-ubyte.gz', 'MNIST_Dataset/train-labels-idx1-ubyte.gz')\n",
    "test_X, test_Y = [np.transpose(a) for a in get_data(test_size, 'MNIST_Dataset/t10k-images-idx3-ubyte.gz', 'MNIST_Dataset/t10k-labels-idx1-ubyte.gz')]\n",
    "\n",
    "# print(np.min(train_X))\n",
    "# print(np.max(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784, 200, 50, 10]\n",
    "batch_size = 10000\n",
    "eta = 1.0\n",
    "epochs = 100\n",
    "\n",
    "network = Network(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    train_X, train_Y = shuffle(train_X, train_Y)\n",
    "    network.train(np.transpose(train_X), np.transpose(train_Y), eta, batch_size)\n",
    "    network.getAccuracy(test_X, test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
